{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Speeds - Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dask.array as dsa\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import dask\n",
    "import intake\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.colors\n",
    "import pandas as pd\n",
    "from scipy.stats import sem\n",
    "import tiledb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/cloud-data/lib/python3.10/site-packages/distributed/node.py:180: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 36225 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>Scheduler: </strong>tcp://127.0.0.1:37671\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> workers: </span>8\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> cores: </span>16\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> tasks: </span>0"
      ],
      "text/plain": [
       "<Scheduler 'tcp://127.0.0.1:37671', workers: 8, cores: 16, tasks: 0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(threads_per_worker=2)\n",
    "client = Client(cluster)\n",
    "cluster.scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is only used for benchmarking reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevNullStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __setitem__(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "null_store = DevNullStore()\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        self.names = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"runtime\"] = toc - tic\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        return pd.DataFrame(self.diagnostics)\n",
    "    \n",
    "diag_timer = DiagnosticTimer()\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "def name(fileType, daf): \n",
    "    globals()[f\"df_{fileType}\"] = daf\n",
    "    diag_timer.names.append(globals()[f\"df_{fileType}\"])\n",
    "    \n",
    "    global df, da\n",
    "    del df, da\n",
    "    diag_timer.diagnostics = []\n",
    "    \n",
    "############################################################################################################################   \n",
    "\n",
    "def total_nthreads():\n",
    "    return sum([v for v in client.nthreads().values()])\n",
    "\n",
    "def total_ncores():\n",
    "    return sum([v for v in client.ncores().values()])\n",
    "\n",
    "def total_workers():\n",
    "    return len(client.ncores())\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "class mainLoop:\n",
    "    def errorCalc(self, df0):\n",
    "        global tests\n",
    "        newVals = []\n",
    "        info = []\n",
    "        thrPut = df0['throughput_Mbps']\n",
    "        rTime = df0['runtime']\n",
    "        for i in np.linspace(0, len(thrPut)-tests, int(len(thrPut)/tests), dtype='int'):\n",
    "            means = thrPut[slice(i,(i+tests))].mean()\n",
    "            runtime = rTime[slice(i,(i+tests))].mean()\n",
    "            errors = sem(thrPut[slice(i,(i+tests))])\n",
    "            error_kwargs = dict(runtime = runtime, throughput_Mbps = means, errors = errors)\n",
    "            info.append(df0.iloc[i, 0:7])\n",
    "            newVals.append(error_kwargs)\n",
    "        \n",
    "        df1 = pd.DataFrame(info, index=range(len(info)))\n",
    "        df2 = pd.DataFrame(newVals)\n",
    "        df = pd.concat([df1, df2], axis=1)\n",
    "        return df\n",
    "\n",
    "    def loop(self, da, diag_kwargs):\n",
    "        global tests, max_workers, worker_step\n",
    "        for nworkers in np.flip(np.arange(max_workers, 0, -worker_step)):\n",
    "            cluster.scale(nworkers)\n",
    "            time.sleep(10)\n",
    "            client.wait_for_workers(nworkers)\n",
    "            print('Number of Workers:', nworkers)\n",
    "            for i in range(tests):\n",
    "                with diag_timer.time(nworkers=total_workers(), nthreads=total_nthreads(), ncores=total_ncores(),\n",
    "                                     **diag_kwargs):\n",
    "                    future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "                    dask.compute(future, retries=5)\n",
    "                del future\n",
    "        \n",
    "        df = diag_timer.dataframe()\n",
    "        df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "        if i != 0:\n",
    "            df = self.errorCalc(df)\n",
    "        return df\n",
    "\n",
    "mainLoop = mainLoop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop Parameters\n",
    "tests = 5\n",
    "max_workers = 8\n",
    "worker_step = 1\n",
    "\n",
    "# Data Location (DO NOT CHANGE THESE)\n",
    "root = 'gs://cloud-data-benchmarks/'\n",
    "data = 'slp.1948-2009.100MB'\n",
    "variable = 'SLP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Script\n",
    "\n",
    "This bucket does not have public write access. I attempted to use the `commits` consolidation and vacuuming feature, but I do not have the most up to date version of TileDB-Py installed in my VM image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tiledb.Config()\n",
    "config['vfs.gcs.project_id'] = 'modular-magpie-167320' # Input your project ID here\n",
    "ctx = tiledb.Ctx(config)\n",
    "\n",
    "uri = root + data + '.tldb'\n",
    "\n",
    "with diag_timer.time(conversionType='netcdf2tldb'):\n",
    "    ds = intake.open_netcdf(root + data + '.nc').to_dask()\n",
    "    da = ds[variable]\n",
    "    internal_chunks = da.encoding['chunksizes']\n",
    "    coords = da.dims\n",
    "    da = da.chunk(chunks=dict(zip(coords, internal_chunks))).data\n",
    "\n",
    "############################################################################################################################\n",
    "    # TileDB Custom Schema Creation\n",
    "\n",
    "    filter_list = tiledb.FilterList([tiledb.LZ4Filter(level=5)])\n",
    "\n",
    "    dims = []\n",
    "    for n in range(len(coords)):\n",
    "        dim = tiledb.Dim(name=coords[n], domain=(0, ds[variable].encoding['original_shape'][n]-1),\n",
    "                         tile=internal_chunks[n], dtype=np.uint64, filters=filter_list)\n",
    "        dims.append(dim)\n",
    "\n",
    "    attr = [tiledb.Attr(name=variable, dtype=np.float32, filters=filter_list)]\n",
    "    dom = tiledb.Domain(dims)\n",
    "\n",
    "    schema = tiledb.ArraySchema(domain=dom, attrs=attr, sparse=False)\n",
    "    tiledb.Array.create(uri, schema)\n",
    "    tdb_array = tiledb.open(uri, \"w\")\n",
    "############################################################################################################################\n",
    "\n",
    "    # The two configuration options were chosen as 2 to coincide with the threads_per_worker value set before\n",
    "    da.to_tiledb(tdb_array, storage_options={\"sm.compute_concurrency_level\": 2, \"sm.io_concurrency_level \": 2})\n",
    "\n",
    "    config['sm.consolidation.mode'] = 'fragment_meta'\n",
    "    ctx = tiledb.Ctx(config)\n",
    "    tiledb.consolidate(uri, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results = diag_timer.dataframe()\n",
    "diag_timer.diagnostics = [] # Clear write diagnostics for use in read throughput benchmarking\n",
    "write_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "da = dsa.from_tiledb(root + data + '.tldb')\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='TileDB Embedded', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('tldb', df)\n",
    "df_tldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Dask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = intake.open_netcdf(root + data + '.nc').to_dask()\n",
    "data = ds[variable].data # This will simply output a numpy array of values, disregard the `.to_dask()` in the first line\n",
    "\n",
    "#################################################Schema Creation############################################################\n",
    "\n",
    "filter_list = tiledb.FilterList([tiledb.LZ4Filter(level=5)])\n",
    "\n",
    "dims = []\n",
    "for n in range(len(coords)):\n",
    "    dim = tiledb.Dim(name=coords[n], domain=(0, ds[variable].encoding['original_shape'][n]-1),\n",
    "                     tile=internal_chunks[n], dtype=np.uint64, filters=filter_list)\n",
    "    dims.append(dim)\n",
    "\n",
    "attr = [tiledb.Attr(name=variable, dtype=np.float32, filters=filter_list)]\n",
    "dom = tiledb.Domain(dims)\n",
    "\n",
    "schema = tiledb.ArraySchema(domain=dom, attrs=attr, sparse=False)\n",
    "tiledb.Array.create(uri, schema)\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "with tiledb.open(uri, \"w\") as tdb_array:\n",
    "    tdb_array = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My entire throughput testing process relies on Dask to scale the number of workers up, so I will work on creating a benchmark for TileDB Embedded without the use of Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell below will give you a throughput plot for the read results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class errorPlot:\n",
    "    def plot(self):\n",
    "        x = self.df['nworkers']\n",
    "        y = self.df['throughput_Mbps']\n",
    "        error = self.df['errors']\n",
    "        plt.errorbar(x, y, yerr=error, color=self.c, fmt='o', capsize=5, capthick=2)\n",
    "        \n",
    "    def errorCheck(self, daf, color):\n",
    "        self.c = color\n",
    "        self.df = daf\n",
    "        try:\n",
    "            self.plot()\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            self.plot()\n",
    "            \n",
    "errorPlot = errorPlot()\n",
    "\n",
    "\n",
    "color = cm.rainbow(np.linspace(0,1,len(diag_timer.names)))\n",
    "legend = []\n",
    "df_results = pd.concat(diag_timer.names, ignore_index=True)\n",
    "\n",
    "for i in range(len(diag_timer.names)):\n",
    "    legend.append(diag_timer.names[i]['format'][1])\n",
    "    c = matplotlib.colors.to_hex(color[i,:], keep_alpha=True)\n",
    "    \n",
    "    if i == 0:\n",
    "        ax = diag_timer.names[i].plot(x='nworkers', y='throughput_Mbps', kind='line', color=c, marker='o')\n",
    "    else:\n",
    "        diag_timer.names[i].plot(x='nworkers', y='throughput_Mbps', kind='line', color=c, ax=ax, marker='o')\n",
    "        \n",
    "    errorPlot.errorCheck(diag_timer.names[i], c) \n",
    "    plt.grid(True)\n",
    "    plt.title('Cloud Data Read Speeds with Dask')\n",
    "    plt.xlabel('Number of Parallel Reads')\n",
    "    plt.ylabel('Throughput (Mbps)')\n",
    "    plt.legend(legend, bbox_to_anchor=[1.25, 0.5], loc='center', title='Store Formats')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    #plt.yscale('symlog') ACTIVATE THIS LINE IF YOU ARE USING A LARGE AMOUNT OF WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH ubuntu@34.148.117.73 linuxpool-cloud-data",
   "language": "",
   "name": "rik_ssh_ubuntu_34_148_117_73_linuxpoolclouddata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
