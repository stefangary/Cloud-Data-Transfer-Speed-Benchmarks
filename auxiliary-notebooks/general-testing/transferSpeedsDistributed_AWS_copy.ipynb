{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Slurm Parallel Data Read Speeds with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mgmt-jacobgreen-awsslurmv2basic2-00012\n"
     ]
    }
   ],
   "source": [
    "import dask.array as dsa\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from contextlib import contextmanager\n",
    "import xarray as xr\n",
    "import intake\n",
    "import time\n",
    "import dask\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.colors\n",
    "import pandas as pd\n",
    "from scipy.stats import sem\n",
    "import tiledb\n",
    "import socket\n",
    "print(socket.gethostname())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slurm Job Script Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda Directory: /var/lib/pworks/miniconda3 \n",
      "Conda Environment: cloud-data\n",
      "Job Script:\n",
      " #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -e /mnt/shared/dask/logs/dask-worker-%J.err\n",
      "#SBATCH -o /mnt/shared/dask/logs/dask-worker-%J.out\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A ca-cloudmgmt\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=9\n",
      "#SBATCH -t 01:00:00\n",
      "source /var/lib/pworks/miniconda3/etc/profile.d/conda.sh; conda activate cloud-data\n",
      "/var/lib/pworks/miniconda3/envs/cloud-data/bin/python -m distributed.cli.dask_worker tcp://10.1.3.83:41195 --nthreads 1 --nprocs 4 --memory-limit 3.73GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "dask_dir = '/mnt/shared/dask'\n",
    "conda_dir = '/var/lib/pworks/miniconda3'\n",
    "conda_env = 'cloud-data'\n",
    "print('Conda Directory:', conda_dir, '\\nConda Environment:', conda_env)\n",
    "\n",
    "cluster = SLURMCluster(project='ca-cloudmgmt',\n",
    "                       cores=4, # Number of cores in the job\n",
    "                       memory='16GB', # Worker memory limit will be memory/processes\n",
    "                       processes=4, # Sets number of Dask workers. Threads per dask worker will be cores/processes\n",
    "                       name='awsv2slurmbasic2', # Name of cluster\n",
    "                       queue='compute', # Partition name\n",
    "                       job_cpu=9, # Set this to the number of cpus per job\n",
    "                       job_mem='48GB', # Amount of memory per job\n",
    "                       walltime='01:00:00',\n",
    "                       log_directory=os.path.join(dask_dir, 'logs'),\n",
    "                       env_extra=[\n",
    "                           'source {conda_sh}; conda activate {conda_env}'.format(\n",
    "                           conda_sh = os.path.join(conda_dir, 'etc/profile.d/conda.sh'),\n",
    "                           conda_env= conda_env\n",
    "                           )\n",
    "                       ],\n",
    "                       header_skip=['--mem'],\n",
    "                      )\n",
    "\n",
    "client = Client(cluster)\n",
    "print('Job Script:\\n',cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevNullStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __setitem__(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "null_store = DevNullStore()\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        self.names = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"runtime\"] = toc - tic\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        return pd.DataFrame(self.diagnostics)\n",
    "    \n",
    "diag_timer = DiagnosticTimer()\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "def name(fileType, daf): \n",
    "    globals()[f\"df_{fileType}\"] = daf\n",
    "    diag_timer.names.append(globals()[f\"df_{fileType}\"])\n",
    "    \n",
    "    global df, da\n",
    "    del df, da\n",
    "    diag_timer.diagnostics = []\n",
    "    \n",
    "############################################################################################################################     \n",
    "\n",
    "def total_nthreads():\n",
    "    return sum([v for v in client.nthreads().values()])\n",
    "\n",
    "def total_ncores():\n",
    "    return sum([v for v in client.ncores().values()])\n",
    "\n",
    "def total_workers():\n",
    "    return len(client.ncores())\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "class mainLoop:\n",
    "    def errorCalc(self, df0):\n",
    "        global tests\n",
    "        newVals = []\n",
    "        info = []\n",
    "        thrPut = df0['throughput_Mbps']\n",
    "        rTime = df0['runtime']\n",
    "        for i in np.linspace(0, len(thrPut)-tests, int(len(thrPut)/tests), dtype='int'):\n",
    "            means = thrPut[slice(i,(i+tests))].mean()\n",
    "            runtime = rTime[slice(i,(i+tests))].mean()\n",
    "            errors = sem(thrPut[slice(i,(i+tests))])\n",
    "            error_kwargs = dict(runtime = runtime, throughput_Mbps = means, errors = errors)\n",
    "            info.append(df0.iloc[i, 0:8])\n",
    "            newVals.append(error_kwargs)\n",
    "        \n",
    "        df1 = pd.DataFrame(info, index=range(len(info)))\n",
    "        df2 = pd.DataFrame(newVals)\n",
    "        df = pd.concat([df1, df2], axis=1)\n",
    "        return df\n",
    "\n",
    "    def loop(self, da, diag_kwargs):\n",
    "        global tests, max_workers, worker_step\n",
    "        worker_range = np.arange(max_workers, 0, -worker_step)\n",
    "        worker_range = np.insert(worker_range, 0, max_workers)\n",
    "        for nworkers in worker_range:\n",
    "            cluster.scale(nworkers)\n",
    "            time.sleep(10)\n",
    "            client.wait_for_workers(nworkers)\n",
    "            print('Number of Workers:', nworkers)\n",
    "            for i in range(tests):\n",
    "                with diag_timer.time(nworkers=total_workers(), nthreads=total_nthreads(), ncores=total_ncores(),\n",
    "                                     **diag_kwargs):\n",
    "                    future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "                    dask.compute(future, retries=5)\n",
    "                del future\n",
    "        \n",
    "        df = diag_timer.dataframe()\n",
    "        df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "        if i != 0:\n",
    "            df = self.errorCalc(df)\n",
    "        #df.drop(index=df.index[0], \n",
    "        #axis=0, \n",
    "        #inplace=True)\n",
    "        #df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "mainLoop = mainLoop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  User Input for Testing Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop Parameters\n",
    "tests = 10\n",
    "max_workers = 100\n",
    "worker_step = 4 # Should be the same or a multiple of the number of processes you set in SLURMCluster(...)\n",
    "\n",
    "# Data Location\n",
    "data = 'so'\n",
    "\n",
    "# Cloud Access Information\n",
    "cloud = 'GCP'\n",
    "instance = 'AWS'\n",
    "\n",
    "# Cloud Storage Access Token File\n",
    "if cloud == 'GCP':\n",
    "    root = 'gs://cloud-data-benchmarks/'\n",
    "    storage_options = dict(token='/var/lib/pworks/cloud-data-benchmarks.json')\n",
    "elif cloud == 'AWS':\n",
    "    root = 's3://cloud-data-benchmarks/'\n",
    "    storage_options = dict(dict(key='AKIARQNWZVPEXT4R7FGM', secret='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'), \n",
    "                        config_kwargs=dict(s3=dict(max_concurrent_requests=100)))\n",
    "else:\n",
    "    print('Please specify the cloud provider (either \"GCP\" or \"AWS\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(max_workers)\n",
    "client.wait_for_workers(max_workers)\n",
    "tic1 = time.time()\n",
    "df0 = dd.read_csv(root + data + '.csv', assume_missing=True, names=['lon', 'lat', 'z'], storage_options=storage_options)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "cluster.scale(0)\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                    format='CSV', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('csv', df)\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(max_workers)\n",
    "client.wait_for_workers(max_workers)\n",
    "tic1 = time.time()\n",
    "df0 = dd.read_csv(root + data + '.50MB.partcsv/*', assume_missing=True, names=['lon', 'lat', 'z'],\n",
    "                  storage_options=storage_options)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "cluster.scale(0)\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                    format='Partitioned CSV', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('partcsv', df)\n",
    "df_partcsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(max_workers)\n",
    "client.wait_for_workers(max_workers)\n",
    "tic1 = time.time()\n",
    "df0 = dd.read_parquet(root + data + '.100MB.partparquet/*', storage_options=storage_options)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "cluster.scale(0)\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                    format='Partitioned Parquet', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('partparquet', df)\n",
    "df_partparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.open_netcdf(root + data + '.nc',\n",
    "                   storage_options=storage_options).to_dask().data_vars # Lists all data variables contained in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`variable = (string)` Choose a data variable from the list in the output above to use in read testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'so'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "ds = intake.open_netcdf(root + data + '.nc', storage_options=storage_options).to_dask()\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "# Set Dask chunks to match internal chunks\n",
    "internal_chunks = ds[variable].encoding['chunksizes']\n",
    "coords = ds[variable].dims\n",
    "da = ds[variable].chunk(chunks=dict(zip(coords, internal_chunks))).data\n",
    "\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "del ds\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                    format='NetCDF', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('netcdf', df)\n",
    "df_netcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zarr Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 34.77 GiB </td>\n",
       "                        <td> 107.90 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (1980, 45, 291, 360) </td>\n",
       "                        <td> (6, 45, 291, 360) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 2 Graph Layers </td>\n",
       "                        <td> 330 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float32 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"489\" height=\"108\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"3\" y1=\"0\" x2=\"3\" y2=\"25\" />\n",
       "  <line x1=\"7\" y1=\"0\" x2=\"7\" y2=\"25\" />\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"25\" />\n",
       "  <line x1=\"14\" y1=\"0\" x2=\"14\" y2=\"25\" />\n",
       "  <line x1=\"18\" y1=\"0\" x2=\"18\" y2=\"25\" />\n",
       "  <line x1=\"22\" y1=\"0\" x2=\"22\" y2=\"25\" />\n",
       "  <line x1=\"26\" y1=\"0\" x2=\"26\" y2=\"25\" />\n",
       "  <line x1=\"29\" y1=\"0\" x2=\"29\" y2=\"25\" />\n",
       "  <line x1=\"33\" y1=\"0\" x2=\"33\" y2=\"25\" />\n",
       "  <line x1=\"37\" y1=\"0\" x2=\"37\" y2=\"25\" />\n",
       "  <line x1=\"41\" y1=\"0\" x2=\"41\" y2=\"25\" />\n",
       "  <line x1=\"44\" y1=\"0\" x2=\"44\" y2=\"25\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"25\" />\n",
       "  <line x1=\"52\" y1=\"0\" x2=\"52\" y2=\"25\" />\n",
       "  <line x1=\"56\" y1=\"0\" x2=\"56\" y2=\"25\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"25\" />\n",
       "  <line x1=\"63\" y1=\"0\" x2=\"63\" y2=\"25\" />\n",
       "  <line x1=\"67\" y1=\"0\" x2=\"67\" y2=\"25\" />\n",
       "  <line x1=\"70\" y1=\"0\" x2=\"70\" y2=\"25\" />\n",
       "  <line x1=\"74\" y1=\"0\" x2=\"74\" y2=\"25\" />\n",
       "  <line x1=\"78\" y1=\"0\" x2=\"78\" y2=\"25\" />\n",
       "  <line x1=\"82\" y1=\"0\" x2=\"82\" y2=\"25\" />\n",
       "  <line x1=\"86\" y1=\"0\" x2=\"86\" y2=\"25\" />\n",
       "  <line x1=\"89\" y1=\"0\" x2=\"89\" y2=\"25\" />\n",
       "  <line x1=\"93\" y1=\"0\" x2=\"93\" y2=\"25\" />\n",
       "  <line x1=\"97\" y1=\"0\" x2=\"97\" y2=\"25\" />\n",
       "  <line x1=\"101\" y1=\"0\" x2=\"101\" y2=\"25\" />\n",
       "  <line x1=\"104\" y1=\"0\" x2=\"104\" y2=\"25\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"25\" />\n",
       "  <line x1=\"112\" y1=\"0\" x2=\"112\" y2=\"25\" />\n",
       "  <line x1=\"116\" y1=\"0\" x2=\"116\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1980</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"190\" y1=\"0\" x2=\"207\" y2=\"17\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"190\" y1=\"40\" x2=\"207\" y2=\"58\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"190\" y1=\"0\" x2=\"190\" y2=\"40\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"207\" y1=\"17\" x2=\"207\" y2=\"58\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"190.0,0.0 207.50496594506004,17.504965945060036 207.50496594506004,58.11238481092921 190.0,40.60741886586917\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"190\" y1=\"0\" x2=\"231\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"207\" y1=\"17\" x2=\"249\" y2=\"17\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"190\" y1=\"0\" x2=\"207\" y2=\"17\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"231\" y1=\"0\" x2=\"249\" y2=\"17\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"190.0,0.0 231.59732749150098,0.0 249.102293436561,17.504965945060036 207.50496594506004,17.504965945060036\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"207\" y1=\"17\" x2=\"249\" y2=\"17\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"207\" y1=\"58\" x2=\"249\" y2=\"58\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"207\" y1=\"17\" x2=\"207\" y2=\"58\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"249\" y1=\"17\" x2=\"249\" y2=\"58\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"207.50496594506004,17.504965945060036 249.102293436561,17.504965945060036 249.102293436561,58.11238481092921 207.50496594506004,58.11238481092921\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"228.303630\" y=\"78.112385\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >360</text>\n",
       "  <text x=\"269.102293\" y=\"37.808675\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,269.102293,37.808675)\">291</text>\n",
       "  <text x=\"188.752483\" y=\"69.359902\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,188.752483,69.359902)\">45</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<from-zarr, shape=(1980, 45, 291, 360), dtype=float32, chunksize=(6, 45, 291, 360), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic1 = time.time()\n",
    "da = dsa.from_zarr(root + data + '.zarray', storage_options=storage_options)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                    format='Zarr Array', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('zarray', df)\n",
    "df_zarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zarr Hierachical Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "ds = xr.open_zarr(store = root + data + '.zarr', consolidated=True, storage_options=storage_options)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "da = ds[variable].data\n",
    "del ds\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                    format='Zarr Group', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('zgroup', df)\n",
    "df_zgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TileDB Embedded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = token\n",
    "tic1 = time.time()\n",
    "da = dsa.from_tiledb(root + data + '.tldb',\n",
    "                     storage_options={'sm.compute_concurrency_level': 4, 'sm.io_concurrency_level': 4})\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                    format='TileDB Embedded', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('tldb', df)\n",
    "df_tldb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.concat(diag_timer.names, ignore_index=True)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH awsslurmv2basic2.clusters.pw awsslurmv2basic2-cloud-data",
   "language": "",
   "name": "rik_ssh_awsslurmv2basic2_clusters_pw_awsslurmv2basic2clouddata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
