{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Slurm Parallel Data Read Speeds with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mgmt-jacobgreen-gcpslurmv2basic-00097\n"
     ]
    }
   ],
   "source": [
    "import dask.array as dsa\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from contextlib import contextmanager\n",
    "import xarray as xr\n",
    "import intake\n",
    "import time\n",
    "import dask\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.colors\n",
    "import pandas as pd\n",
    "from scipy.stats import sem\n",
    "import tiledb\n",
    "import socket\n",
    "print(socket.gethostname())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slurm Job Script Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda Directory: /var/lib/pworks/miniconda3 \n",
      "Conda Environment: cloud-data\n",
      "Job Script:\n",
      " #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -e /mnt/shared/dask/logs/dask-worker-%J.err\n",
      "#SBATCH -o /mnt/shared/dask/logs/dask-worker-%J.out\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A cg-cloudmgmt\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=4\n",
      "#SBATCH -t 01:00:00\n",
      "source /var/lib/pworks/miniconda3/etc/profile.d/conda.sh; conda activate cloud-data\n",
      "/var/lib/pworks/miniconda3/envs/cloud-data/bin/python -m distributed.cli.dask_worker tcp://10.150.0.53:39260 --nthreads 1 --nprocs 4 --memory-limit 3.73GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "dask_dir = '/mnt/shared/dask'\n",
    "conda_dir = '/var/lib/pworks/miniconda3'\n",
    "conda_env = 'cloud-data'\n",
    "print('Conda Directory:', conda_dir, '\\nConda Environment:', conda_env)\n",
    "\n",
    "cluster = SLURMCluster(project='cg-cloudmgmt',\n",
    "                       cores=4, # Number of cores in the job\n",
    "                       memory='16GB', # Worker memory limit will be memory/processes\n",
    "                       processes=4, # Sets number of Dask workers. Threads per dask worker will be cores/processes\n",
    "                       name='gcpslurmv2basic', # Name of cluster\n",
    "                       queue='compute', # Partition name\n",
    "                       job_cpu=4, # Set this to the number of cpus per job\n",
    "                       job_mem='16GB', # Amount of memory per job\n",
    "                       walltime='01:00:00',\n",
    "                       log_directory=os.path.join(dask_dir, 'logs'),\n",
    "                       env_extra=[\n",
    "                           'source {conda_sh}; conda activate {conda_env}'.format(\n",
    "                           conda_sh = os.path.join(conda_dir, 'etc/profile.d/conda.sh'),\n",
    "                           conda_env= conda_env\n",
    "                           )\n",
    "                       ],\n",
    "                       header_skip=['--mem'],\n",
    "                      )\n",
    "\n",
    "client = Client(cluster)\n",
    "print('Job Script:\\n',cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevNullStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __setitem__(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "null_store = DevNullStore()\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        self.names = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"runtime\"] = toc - tic\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        return pd.DataFrame(self.diagnostics)\n",
    "    \n",
    "diag_timer = DiagnosticTimer()\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "def name(fileType, daf): \n",
    "    globals()[f\"df_{fileType}\"] = daf\n",
    "    diag_timer.names.append(globals()[f\"df_{fileType}\"])\n",
    "    \n",
    "    global df, da\n",
    "    del df, da\n",
    "    diag_timer.diagnostics = []\n",
    "    \n",
    "############################################################################################################################     \n",
    "\n",
    "def total_nthreads():\n",
    "    return sum([v for v in client.nthreads().values()])\n",
    "\n",
    "def total_ncores():\n",
    "    return sum([v for v in client.ncores().values()])\n",
    "\n",
    "def total_workers():\n",
    "    return len(client.ncores())\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "class mainLoop:\n",
    "    def errorCalc(self, df0):\n",
    "        global tests\n",
    "        newVals = []\n",
    "        info = []\n",
    "        thrPut = df0['throughput_Mbps']\n",
    "        rTime = df0['runtime']\n",
    "        for i in np.linspace(0, len(thrPut)-tests, int(len(thrPut)/tests), dtype='int'):\n",
    "            means = thrPut[slice(i,(i+tests))].mean()\n",
    "            runtime = rTime[slice(i,(i+tests))].mean()\n",
    "            errors = sem(thrPut[slice(i,(i+tests))])\n",
    "            error_kwargs = dict(runtime = runtime, throughput_Mbps = means, errors = errors)\n",
    "            info.append(df0.iloc[i, 0:8])\n",
    "            newVals.append(error_kwargs)\n",
    "        \n",
    "        df1 = pd.DataFrame(info, index=range(len(info)))\n",
    "        df2 = pd.DataFrame(newVals)\n",
    "        df = pd.concat([df1, df2], axis=1)\n",
    "        return df\n",
    "\n",
    "    def loop(self, da, diag_kwargs):\n",
    "        global tests, max_workers, worker_step\n",
    "        worker_range = np.arange(max_workers, 0, -worker_step)\n",
    "        worker_range = np.insert(worker_range,0, max_workers)\n",
    "        for nworkers in worker_range:\n",
    "            cluster.scale(nworkers)\n",
    "            time.sleep(10)\n",
    "            client.wait_for_workers(nworkers)\n",
    "            print('Number of Workers:', nworkers)\n",
    "            for i in range(tests):\n",
    "                with diag_timer.time(nworkers=total_workers(), nthreads=total_nthreads(), ncores=total_ncores(),\n",
    "                                     **diag_kwargs):\n",
    "                    future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "                    dask.compute(future, retries=5)\n",
    "                del future\n",
    "        \n",
    "        df = diag_timer.dataframe()\n",
    "        df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "        if i != 0:\n",
    "            df = self.errorCalc(df)\n",
    "        #df.drop(index=df.index[0], \n",
    "        #axis=0, \n",
    "        #inplace=True)\n",
    "        #df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "mainLoop = mainLoop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  User Input for Testing Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the user will define the testing benchmarking conditions:\n",
    "* **`tests = (int)`:** The number of times each individual file format will be read for measurement. Entering a number greater than 1 will take much longer to run, but results will include errors & throughput plot will have error bars.\n",
    "\n",
    "\n",
    "* **`max_workers = (int)`:** Maximum amount of parallel reads to be tested.\n",
    "\n",
    "\n",
    "* **`worker_step = (int)`:** Workers will be reduced by this number starting from the value of `max_workers` until the lowest possible value is reached. For instance, when `max_workers = 8` & `worker_step = 2`, the resulting worker scaling scheme will be `[2, 4, 6, 8]`.\n",
    "\n",
    "\n",
    "* **`data = (string)`:** The data set to test. Within the `gs://cloud-data-benchmarks` bucket, each file format begins with the same naming convention, with the only difference being the extension at the end of the file name -- e.g. `.nc`, `.zarr`, etc. If the user is providing their own data and bucket, ensure that the naming convention follows what was done for this notebook, or hardcode the object storage URIs in each applicable function call.\n",
    "\n",
    "\n",
    "* **`cloud = (string)`:** Specify either `'GCP'` or `'AWS'`. Based on the choice, the corresponding credentials and URI will be set.\n",
    "\n",
    "\n",
    "* **`instance = (string)`:** Note down the VM instance provider used for the worker nodes of the cluster. This will be recorded in the benchmarking output data.\n",
    "\n",
    "Note: When using a data set that only has gridded formats available in cloud object storage, only run the **Gridded Data** section of the notebook. The **Tabluar Data** section will *not* work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop Parameters\n",
    "tests = 5\n",
    "max_workers = 40\n",
    "worker_step = 4 # Should be the same or a multiple of the number of processes you set in SLURMCluster(...)\n",
    "\n",
    "# Data Location\n",
    "data = 'slp.1948-2009'\n",
    "\n",
    "# Cloud Access Information\n",
    "cloud = 'AWS'\n",
    "instance = 'GCP'\n",
    "\n",
    "# Cloud Storage Access Token File\n",
    "if cloud == 'GCP':\n",
    "    root = 'gs://cloud-data-benchmarks/'\n",
    "    storage_options = dict(token='/var/lib/pworks/cloud-data-benchmarks.json')\n",
    "elif cloud == 'AWS':\n",
    "    root = 's3://cloud-data-benchmarks/'\n",
    "    storage_options = dict(key='AKIARQNWZVPEXT4R7FGM', secret='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',\n",
    "                           config_kwargs=dict(s3=dict(max_concurrent_requests=100)))\n",
    "else:\n",
    "    print('Please specify the cloud provider (either \"GCP\" or \"AWS\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(max_workers)\n",
    "client.wait_for_workers(max_workers)\n",
    "tic1 = time.time()\n",
    "df0 = dd.read_csv(root + data + '.csv', assume_missing=True, names=['lon', 'lat', 'z'], storage_options=storage_options)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "cluster.scale(0)\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                   format='CSV', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('csv', df)\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(max_workers)\n",
    "client.wait_for_workers(max_workers)\n",
    "tic1 = time.time()\n",
    "df0 = dd.read_csv(root + data + '.partcsv/*', assume_missing=True, names=['lon', 'lat', 'z'],\n",
    "                  storage_options=storage_options)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "cluster.scale(0)\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                   format='Partitioned CSV', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('partcsv', df)\n",
    "df_partcsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(max_workers)\n",
    "client.wait_for_workers(max_workers)\n",
    "tic1 = time.time()\n",
    "df0 = dd.read_parquet(root + data + '.partparquet/*', storage_options=storage_options)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "cluster.scale(0)\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                   format='Partitioned Parquet', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('partparquet', df)\n",
    "df_partparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data variables:\n",
       "    SLP      (TIME, LAT, LON) float32 ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intake.open_netcdf(root + data + '.nc',\n",
    "                   storage_options=storage_options).to_dask().data_vars # Lists all data variables contained in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`variable = (string)` Choose a data variable from the list in the output above to use in read testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'SLP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "ds = intake.open_netcdf(root + data + '.nc', storage_options=storage_options).to_dask()\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "# Set Dask chunks to match internal chunks\n",
    "internal_chunks = ds[variable].encoding['chunksizes']\n",
    "coords = ds[variable].dims\n",
    "da = ds[variable].chunk(chunks=dict(zip(coords, internal_chunks))).data\n",
    "\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "del ds\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                   format='NetCDF', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('netcdf', df)\n",
    "df_netcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zarr Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 6.09 GiB </td>\n",
       "                        <td> 99.45 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (90520, 94, 192) </td>\n",
       "                        <td> (22630, 24, 48) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 2 Graph Layers </td>\n",
       "                        <td> 64 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float32 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"156\" height=\"146\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"6\" x2=\"80\" y2=\"77\" />\n",
       "  <line x1=\"10\" y1=\"12\" x2=\"80\" y2=\"83\" />\n",
       "  <line x1=\"10\" y1=\"19\" x2=\"80\" y2=\"90\" />\n",
       "  <line x1=\"10\" y1=\"25\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"43\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"45\" y2=\"60\" />\n",
       "  <line x1=\"62\" y1=\"52\" x2=\"62\" y2=\"78\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 80.58823529411765,70.58823529411765 80.58823529411765,96.00085180870013 10.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"35\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"53\" y2=\"17\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"70\" y2=\"35\" />\n",
       "  <line x1=\"62\" y1=\"52\" x2=\"88\" y2=\"52\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"16\" y1=\"0\" x2=\"86\" y2=\"70\" />\n",
       "  <line x1=\"22\" y1=\"0\" x2=\"93\" y2=\"70\" />\n",
       "  <line x1=\"29\" y1=\"0\" x2=\"99\" y2=\"70\" />\n",
       "  <line x1=\"35\" y1=\"0\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 35.41261651458248,0.0 106.00085180870013,70.58823529411765 80.58823529411765,70.58823529411765\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"77\" x2=\"106\" y2=\"77\" />\n",
       "  <line x1=\"80\" y1=\"83\" x2=\"106\" y2=\"83\" />\n",
       "  <line x1=\"80\" y1=\"90\" x2=\"106\" y2=\"90\" />\n",
       "  <line x1=\"80\" y1=\"96\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"86\" y1=\"70\" x2=\"86\" y2=\"96\" />\n",
       "  <line x1=\"93\" y1=\"70\" x2=\"93\" y2=\"96\" />\n",
       "  <line x1=\"99\" y1=\"70\" x2=\"99\" y2=\"96\" />\n",
       "  <line x1=\"106\" y1=\"70\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.58823529411765,70.58823529411765 106.00085180870013,70.58823529411765 106.00085180870013,96.00085180870013 80.58823529411765,96.00085180870013\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"93.294544\" y=\"116.000852\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >192</text>\n",
       "  <text x=\"126.000852\" y=\"83.294544\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,126.000852,83.294544)\">94</text>\n",
       "  <text x=\"35.294118\" y=\"80.706734\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,80.706734)\">90520</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<from-zarr, shape=(90520, 94, 192), dtype=float32, chunksize=(22630, 24, 48), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic1 = time.time()\n",
    "da = dsa.from_zarr(root + data + '.zarray', storage_options=storage_options)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Workers: 40\n",
      "Number of Workers: 40\n",
      "Number of Workers: 36\n",
      "Number of Workers: 32\n",
      "Number of Workers: 28\n",
      "Number of Workers: 24\n",
      "Number of Workers: 20\n",
      "Number of Workers: 16\n",
      "Number of Workers: 12\n",
      "Number of Workers: 8\n",
      "Number of Workers: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nworkers</th>\n",
       "      <th>nthreads</th>\n",
       "      <th>ncores</th>\n",
       "      <th>nbytes</th>\n",
       "      <th>chunksize</th>\n",
       "      <th>cloud</th>\n",
       "      <th>instance</th>\n",
       "      <th>format</th>\n",
       "      <th>runtime</th>\n",
       "      <th>throughput_Mbps</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>AWS</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr Array</td>\n",
       "      <td>6.616364</td>\n",
       "      <td>1392.348953</td>\n",
       "      <td>304.887896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>AWS</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr Array</td>\n",
       "      <td>1.796369</td>\n",
       "      <td>3640.054072</td>\n",
       "      <td>45.179714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>AWS</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr Array</td>\n",
       "      <td>1.932696</td>\n",
       "      <td>3478.746118</td>\n",
       "      <td>257.114982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>AWS</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr Array</td>\n",
       "      <td>2.116920</td>\n",
       "      <td>3131.210432</td>\n",
       "      <td>182.402473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>AWS</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr Array</td>\n",
       "      <td>2.311822</td>\n",
       "      <td>2831.452343</td>\n",
       "      <td>56.406180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>AWS</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr Array</td>\n",
       "      <td>4.086520</td>\n",
       "      <td>1692.877530</td>\n",
       "      <td>213.704943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>AWS</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr Array</td>\n",
       "      <td>2.942461</td>\n",
       "      <td>2221.230899</td>\n",
       "      <td>14.096826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>AWS</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr Array</td>\n",
       "      <td>3.294988</td>\n",
       "      <td>1989.126556</td>\n",
       "      <td>54.221706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>AWS</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr Array</td>\n",
       "      <td>4.340678</td>\n",
       "      <td>1505.577927</td>\n",
       "      <td>5.946004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>AWS</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr Array</td>\n",
       "      <td>5.855980</td>\n",
       "      <td>1116.230941</td>\n",
       "      <td>9.233310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>AWS</td>\n",
       "      <td>GCP</td>\n",
       "      <td>Zarr Array</td>\n",
       "      <td>11.396849</td>\n",
       "      <td>573.390851</td>\n",
       "      <td>0.605639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nworkers  nthreads  ncores      nbytes  chunksize cloud instance  \\\n",
       "0         40        40      40  6534819840  104279040   AWS      GCP   \n",
       "1         40        40      40  6534819840  104279040   AWS      GCP   \n",
       "2         36        36      36  6534819840  104279040   AWS      GCP   \n",
       "3         32        32      32  6534819840  104279040   AWS      GCP   \n",
       "4         28        28      28  6534819840  104279040   AWS      GCP   \n",
       "5         24        24      24  6534819840  104279040   AWS      GCP   \n",
       "6         20        20      20  6534819840  104279040   AWS      GCP   \n",
       "7         16        16      16  6534819840  104279040   AWS      GCP   \n",
       "8         12        12      12  6534819840  104279040   AWS      GCP   \n",
       "9          8         8       8  6534819840  104279040   AWS      GCP   \n",
       "10         4         4       4  6534819840  104279040   AWS      GCP   \n",
       "\n",
       "        format    runtime  throughput_Mbps      errors  \n",
       "0   Zarr Array   6.616364      1392.348953  304.887896  \n",
       "1   Zarr Array   1.796369      3640.054072   45.179714  \n",
       "2   Zarr Array   1.932696      3478.746118  257.114982  \n",
       "3   Zarr Array   2.116920      3131.210432  182.402473  \n",
       "4   Zarr Array   2.311822      2831.452343   56.406180  \n",
       "5   Zarr Array   4.086520      1692.877530  213.704943  \n",
       "6   Zarr Array   2.942461      2221.230899   14.096826  \n",
       "7   Zarr Array   3.294988      1989.126556   54.221706  \n",
       "8   Zarr Array   4.340678      1505.577927    5.946004  \n",
       "9   Zarr Array   5.855980      1116.230941    9.233310  \n",
       "10  Zarr Array  11.396849       573.390851    0.605639  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                    format='Zarr Array', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('zarray', df)\n",
    "df_zarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zarr Hierachical Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "ds = xr.open_zarr(store = root + data + '.zarr', consolidated=True, storage_options=storage_options)\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "da = ds[variable].data\n",
    "del ds\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                    format='Zarr Group', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('zgroup', df)\n",
    "df_zgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TileDB Embedded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = token\n",
    "tic1 = time.time()\n",
    "da = dsa.from_tiledb(root + data + '.tldb',\n",
    "                     storage_options={'sm.compute_concurrency_level': 4, 'sm.io_concurrency_level': 4})\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, cloud=cloud, instance=instance,\n",
    "                    format='TileDB Embedded', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('tldb', df)\n",
    "df_tldb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.concat(diag_timer.names, ignore_index=True)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH gcpslurmv2basic.clusters.pw gcpslurmv2basic-cloud-data",
   "language": "",
   "name": "rik_ssh_gcpslurmv2basic_clusters_pw_gcpslurmv2basicclouddata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
