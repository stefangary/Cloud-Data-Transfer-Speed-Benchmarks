{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a84f6f1",
   "metadata": {},
   "source": [
    "# File Transformations - Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import dask.array as dsa\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import intake\n",
    "from contextlib import contextmanager\n",
    "import tiledb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "dask_dir = '/mnt/shared/dask'\n",
    "conda_dir = '/var/lib/pworks/miniconda3'\n",
    "conda_env = 'cloud-data'\n",
    "print('Conda Directory:', conda_dir, '\\nConda Environment:', conda_env)\n",
    "\n",
    "cluster = SLURMCluster(project='cg-cloudmgmt',\n",
    "                       cores=4, # Number of cores in the job\n",
    "                       memory='16GB', # Worker memory limit will be memory/processes\n",
    "                       processes=4, # Sets number of Dask workers. Threads per dask worker will be cores/processes\n",
    "                       name='gcpslurmv2basic', # Name of cluster\n",
    "                       queue='compute', # Partition name\n",
    "                       job_cpu=4, # Set this to the number of cpus per job\n",
    "                       job_mem='16GB', # Amount of memory per job\n",
    "                       walltime='01:00:00',\n",
    "                       log_directory=os.path.join(dask_dir, 'logs'),\n",
    "                       env_extra=[\n",
    "                           'source {conda_sh}; conda activate {conda_env}'.format(\n",
    "                           conda_sh = os.path.join(conda_dir, 'etc/profile.d/conda.sh'),\n",
    "                           conda_env= conda_env\n",
    "                           )\n",
    "                       ],\n",
    "                       header_skip=['--mem'],\n",
    "                      )\n",
    "\n",
    "client = Client(cluster)\n",
    "print('Job Script:\\n',cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef8558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/contrib/Jacob.Green/cloud-data-benchmarks.json\"\n",
    "#token = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "token = '/var/lib/pworks/cloud-data-benchmarks.json'\n",
    "\n",
    "# Bucket name/public URL that contains the data you would like to convert & data set\n",
    "root = 'gs://cloud-data-benchmarks/'\n",
    "data = 'slp.1948-2009'\n",
    "\n",
    "path = root + data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3854e0",
   "metadata": {},
   "source": [
    "## Timing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"Preprocessing Time\"] = toc - tic\n",
    "        kwargs\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        df = pd.DataFrame(self.diagnostics)\n",
    "        return df\n",
    "    \n",
    "diag_timer = DiagnosticTimer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e7fc0d",
   "metadata": {},
   "source": [
    "## Tabular Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e1e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names to give CSV columns. If the file does not have column names, Dask/Pandas will use your first line of data as such.\n",
    "names=['lon', 'lat', 'z']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7edb59",
   "metadata": {},
   "source": [
    "### CSV to Partitioned Parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439452b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_function = lambda x: f\"ETOPO1_Ice_g_gmt4_{x}.parquet\"\n",
    "partition_sizes = ['100MB', '150MB', '500MB'] # For this particular data set, these will output 50MB, 100MB, & 500MB sizes\n",
    "\n",
    "for i in partition_sizes:\n",
    "    with diag_timer.time(conversionType='csv2partparqet/' + i):\n",
    "        df = dd.read_csv(path + '.csv', assume_missing=True, header=None, names=names, storage_options={'token':token})\n",
    "        df = df.repartition(partition_size=i)\n",
    "        dd.to_parquet(df, path + '.' + i + '.partparquet2', name_function=name_function, storage_options={'token':token})\n",
    "    \n",
    "    del df\n",
    "    \n",
    "os.system('gsutil mv gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.100MB.partparquet2 ' + \n",
    "             'gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.50MB.partparquet2')\n",
    "os.system('gsutil mv gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.150MB.partparquet2 ' + \n",
    "             'gs://cloud-data-benchmarks/ETOPO1_Ice_g_gmt4.100MB.partparquet2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88e54c0",
   "metadata": {},
   "source": [
    "### CSV to Partitioned CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_function(i):\n",
    "    return \"ETOPO1_Ice_g_gmt4_\" + str(i) + \".csv\"\n",
    "\n",
    "with diag_timer.time(conversionType='csv2partcsv'):\n",
    "    df = dd.read_csv(path + '.csv', assume_missing=True)\n",
    "    \n",
    "    #df = df.repartition(partition_size='500MB') # Only use this line if you wish to change chunksize\n",
    "    \n",
    "    dd.to_csv(df, path + '.partcsv', name_function=name_function, storage_options={'token':token},\n",
    "              header_first_partition_only=True)\n",
    "    \n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b803c89",
   "metadata": {},
   "source": [
    "## Gridded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.open_netcdf(path + '.100MB.nc', storage_options={'token':token}).to_dask().data_vars \n",
    "# Lists all data variables contained in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c2eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'SLP'\n",
    "labels = ['50MB', '100MB', '500MB'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb7aed",
   "metadata": {},
   "source": [
    "### Zarr Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef2d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in labels:\n",
    "    print('Writing Size', i)\n",
    "    cluster.scale(40)\n",
    "    client.wait_for_workers(40)\n",
    "    with diag_timer.time(conversionType='netcdf2zgroup.' + i):\n",
    "        ds = intake.open_netcdf(path + '.' + i + '.nc', storage_options={'token':token}).to_dask()\n",
    "        da = ds[variable]\n",
    "        internal_chunks = da.encoding['chunksizes']\n",
    "        coords = da.dims\n",
    "        da = da.chunk(chunks=dict(zip(coords, internal_chunks)))\n",
    "        ds = da.to_dataset()\n",
    "        ds.to_zarr(store= path + '.' + i + '.zarr2', storage_options={'token':token}, consolidated=True)\n",
    "    cluster.scale(0)\n",
    "    del ds, da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04f014",
   "metadata": {},
   "source": [
    "### Zarr Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in labels:\n",
    "    print('Writing Size', n)\n",
    "    cluster.scale(40)\n",
    "    client.wait_for_workers(40)\n",
    "    with diag_timer.time(conversionType='netcdf2zarray.' + n):\n",
    "        ds = intake.open_netcdf(path + '.' + n + '.nc', storage_options={'token':token}).to_dask()\n",
    "        da = ds[variable] # Change the variable name as needed\n",
    "        internal_chunks = da.encoding['chunksizes']\n",
    "        coords = da.dims\n",
    "        da = da.chunk(chunks=dict(zip(coords, internal_chunks))).data\n",
    "        dsa.to_zarr(da, path + '.' + n + '.zarray2', storage_options={'token':token})\n",
    "    cluster.scale(0)\n",
    "    del ds, da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1df18",
   "metadata": {},
   "source": [
    "### NetCDF to TileDB Embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76bf39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tiledb.Config()\n",
    "config['vfs.gcs.project_id'] = 'modular-magpie-167320' # Input your project ID here\n",
    "ctx = tiledb.Ctx(config)\n",
    "filters = [tiledb.LZ4Filter(level=5)]\n",
    "\n",
    "for i in labels:\n",
    "    uri = path + '.' + i + '.tldb'\n",
    "    \n",
    "    with diag_timer.time(conversionType='netcdf2tldb.' + i):\n",
    "        ds = intake.open_netcdf(path + '.'+ i + '.nc').to_dask()\n",
    "        da = ds[variable]\n",
    "        internal_chunks = da.encoding['chunksizes']\n",
    "        coords = da.dims\n",
    "        da = da.chunk(chunks=dict(zip(coords, internal_chunks))).data\n",
    "        \n",
    "############################################################################################################################\n",
    "        # TileDB Custom Schema Creation\n",
    "        \n",
    "        filter_list = tiledb.FilterList(filters)\n",
    "        \n",
    "        dims = []\n",
    "        for n in range(len(coords)):\n",
    "            dim = tiledb.Dim(name=coords[n], domain=(0, ds[variable].encoding['original_shape'][n]-1),\n",
    "                             tile=internal_chunks[n], dtype=np.uint64, filters=filter_list)\n",
    "            dims.append(dim)\n",
    "            \n",
    "        attr = [tiledb.Attr(name=variable, dtype=np.float32, filters=filter_list)]\n",
    "        dom = tiledb.Domain(dims)\n",
    "        \n",
    "        schema = tiledb.ArraySchema(domain=dom, attrs=attr, sparse=False)\n",
    "        tiledb.Array.create(uri, schema)\n",
    "        tdb_array = tiledb.open(uri, \"w\")\n",
    "############################################################################################################################\n",
    "        \n",
    "        da.to_tiledb(tdb_array, storage_options={\"sm.compute_concurrency_level\": 2, \"sm.io_concurrency_level \": 2})\n",
    "    \n",
    "        # Consolidation is perfomed on the array for increased read speed from cloud object storage.\n",
    "        config['sm.consolidation.mode'] = 'fragment_meta'\n",
    "        ctx = tiledb.Ctx(config)\n",
    "        tiledb.consolidate(uri, ctx=ctx)\n",
    "        config['sm.consolidation.mode'] = 'fragments'\n",
    "        ctx = tiledb.Ctx(config)\n",
    "        tiledb.consolidate(uri, ctx=ctx)\n",
    "        config['sm.consolidation.mode'] = 'array_meta'\n",
    "        ctx = tiledb.Ctx(config)\n",
    "        tiledb.consolidate(uri, ctx=ctx)\n",
    "    \n",
    "    del ds, da, uri, dims, attr, dom, schema, tdb_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363d30ea",
   "metadata": {},
   "source": [
    "## Present Timing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b059cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH gcpslurmv2basic.clusters.pw gcpslurmv2basic-cloud-data",
   "language": "",
   "name": "rik_ssh_gcpslurmv2basic_clusters_pw_gcpslurmv2basicclouddata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
