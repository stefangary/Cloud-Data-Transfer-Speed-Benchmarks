{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e6f1c8",
   "metadata": {},
   "source": [
    "# File Transformations - Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import dask.array as dsa\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import intake\n",
    "from contextlib import contextmanager\n",
    "import tiledb\n",
    "import os\n",
    "import socket\n",
    "print(socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "dask_dir = '/mnt/shared/dask'\n",
    "conda_dir = '/var/lib/pworks/miniconda3'\n",
    "conda_env = 'cloud-data'\n",
    "print('Conda Directory:', conda_dir, '\\nConda Environment:', conda_env)\n",
    "\n",
    "cluster = SLURMCluster(project='cg-cloudmgmt',\n",
    "                       cores=4, # Number of cores in the job\n",
    "                       memory='16GB', # Worker memory limit will be memory/processes\n",
    "                       processes=4, # Sets number of Dask workers. Threads per dask worker will be cores/processes\n",
    "                       name='gcpslurmv2basic', # Name of cluster\n",
    "                       queue='compute', # Partition name\n",
    "                       job_cpu=4, # Set this to the number of cpus per job\n",
    "                       job_mem='16GB', # Amount of memory per job\n",
    "                       walltime='01:00:00',\n",
    "                       log_directory=os.path.join(dask_dir, 'logs'),\n",
    "                       env_extra=[\n",
    "                           'source {conda_sh}; conda activate {conda_env}'.format(\n",
    "                           conda_sh = os.path.join(conda_dir, 'etc/profile.d/conda.sh'),\n",
    "                           conda_env= conda_env\n",
    "                           )\n",
    "                       ],\n",
    "                       header_skip=['--mem'],\n",
    "                      )\n",
    "\n",
    "client = Client(cluster)\n",
    "print('Job Script:\\n',cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/var/lib/pworks/cloud-data-benchmarks.json\"\n",
    "token = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "# Bucket name/public URL that contains the data you would like to convert & data set\n",
    "root = 'gs://cloud-data-benchmarks/'\n",
    "data = 'ETOPO1_Ice_g_gmt4'\n",
    "\n",
    "path = root + data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251fa221",
   "metadata": {},
   "source": [
    "## Timing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"Preprocessing Time\"] = toc - tic\n",
    "        kwargs\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        df = pd.DataFrame(self.diagnostics)\n",
    "        return df\n",
    "    \n",
    "diag_timer = DiagnosticTimer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a65ee5",
   "metadata": {},
   "source": [
    "## Tabular Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b35ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names to give CSV columns. If the file does not have column names, Dask/Pandas will use your first line of data as such.\n",
    "names=['lon', 'lat', 'z']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e28d3",
   "metadata": {},
   "source": [
    "### CSV to Partitioned Parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_function = lambda x: f\"ETOPO1_Ice_g_gmt4_{x}.parquet\"\n",
    "\n",
    "for i in ['lz4', 'zstd', 'gzip']:\n",
    "    cluster.scale(40)\n",
    "    client.wait_for_workers(40)\n",
    "    with diag_timer.time(conversionType='csv2partparqet/' + i):\n",
    "        df = dd.read_csv(path + '.csv', assume_missing=True, header=None, names=names)\n",
    "        df = df.repartition(partition_size='150MB')\n",
    "        dd.to_parquet(df, path + '.' + i + '.partparquet2', name_function=name_function, storage_options={'token':token},\n",
    "                      compression=i)\n",
    "    cluster.scale(0)\n",
    "\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17c66d",
   "metadata": {},
   "source": [
    "## Gridded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a64ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.open_netcdf(path + '.100MB.nc', storage_options={'token':token}).to_dask().data_vars \n",
    "# Lists all data variables contained in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'SLP'\n",
    "\n",
    "internal_chunks = intake.open_netcdf(path + '.100MB.nc').to_dask()[variable].encoding['chunksizes']\n",
    "coords = intake.open_netcdf(path + '.100MB.nc').to_dask()[variable].dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc6001",
   "metadata": {},
   "source": [
    "### Zarr Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numcodecs import blosc, Blosc, gzip, bz2\n",
    "compressors = blosc.list_compressors()\n",
    "l = 5\n",
    "compressors.remove('lz4') # Zarr was already created with LZ4 compression\n",
    "compressors.append('gzip')\n",
    "compressors.append('bzip2')\n",
    "\n",
    "for i in compressors:\n",
    "    cluster.scale(40)\n",
    "    client.wait_for_workers(40)\n",
    "    if i == 'gzip':\n",
    "        compressor = gzip.GZip(level=l)\n",
    "    elif i == 'bzip2':\n",
    "        compressor = bz2.BZ2(level=l)\n",
    "    else:\n",
    "        compressor = Blosc(cname=i, clevel=l)\n",
    "            \n",
    "    with diag_timer.time(conversionType='netcdf2zarray.' + i):\n",
    "        ds = intake.open_netcdf(path + '.100MB.nc', chunks={}).to_dask()\n",
    "        da = ds[variable]\n",
    "        da = da.chunk(chunks=dict(zip(coords, internal_chunks)))\n",
    "        da = da.data\n",
    "        \n",
    "        dsa.to_zarr(da, path + '.' + i + '.zarray', storage_options={'token':token, 'compressor':compressor})\n",
    "    cluster.scale(0)\n",
    "    del ds, da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5751c",
   "metadata": {},
   "source": [
    "### Zarr Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef169f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numcodecs import blosc, Blosc, gzip, bz2\n",
    "compressors = blosc.list_compressors()\n",
    "l = 5\n",
    "compressors.remove('lz4') # Zarr was already created with LZ4 compression\n",
    "compressors.append('gzip')\n",
    "compressors.append('bzip2')\n",
    "\n",
    "for i in compressors:\n",
    "    cluster.scale(40)\n",
    "    client.wait_for_workers(40)\n",
    "    if i == compressors[-2]:\n",
    "        compressor = gzip.GZip(level=l)\n",
    "    elif i == compressors[-1]:\n",
    "        compressor = bz2.BZ2(level=l)\n",
    "    else:\n",
    "        compressor = zarr.Blosc(cname=i, clevel=l, shuffle=Blosc.SHUFFLE, blocksize=0)\n",
    "            \n",
    "    with diag_timer.time(conversionType='netcdf2zgroup.' + i):\n",
    "        ds = intake.open_netcdf(path + '.100MB.nc').to_dask()\n",
    "        da = ds[variable]\n",
    "        da = da.chunk(chunks=dict(zip(coords, internal_chunks)))\n",
    "        ds = da.to_dataset()\n",
    "        \n",
    "        ds.to_zarr(store= path + '.' + i + '.zarr2', storage_options={'token':token}, consolidated=True,\n",
    "                   encoding={variable: {\"compressor\": compressor}})\n",
    "    cluster.scale(0)\n",
    "    del ds, da\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba148ab",
   "metadata": {},
   "source": [
    "### NetCDF to TileDB Embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f45480",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tiledb.Config()\n",
    "config['vfs.gcs.project_id'] = 'modular-magpie-167320' # Input your project ID here\n",
    "ctx = tiledb.Ctx(config)\n",
    "l = 5 # Compression level input\n",
    "filters = [tiledb.GzipFilter(level=l), tiledb.ZstdFilter(level=l), tiledb.LZ4Filter(level=l), tiledb.Bzip2Filter(level=l)]\n",
    "\n",
    "for i in filters:\n",
    "    uri = path + '.' + str(i).split('F')[0] + '.tldb'\n",
    "    \n",
    "    with diag_timer.time(conversionType='netcdf2tldb.' + str(i).split('F')[0]):\n",
    "        ds = intake.open_netcdf(path + '.100MB.nc').to_dask()\n",
    "        da = ds[variable]\n",
    "        da = da.chunk(chunks=dict(zip(coords, internal_chunks))).data\n",
    "        \n",
    "############################################################################################################################\n",
    "        # TileDB Custom Schema Creation\n",
    "        \n",
    "        filter_list = tiledb.FilterList([i])\n",
    "        \n",
    "        dims = []\n",
    "        for n in range(len(coords)):\n",
    "            dim = tiledb.Dim(name=coords[n], domain=(0, ds[variable].encoding['original_shape'][n]-1),\n",
    "                             tile=internal_chunks[n], dtype=np.uint64, filters=filter_list)\n",
    "            dims.append(dim)\n",
    "            \n",
    "        attr = [tiledb.Attr(name=variable, dtype=np.float32, filters=filter_list)]\n",
    "        dom = tiledb.Domain(dims)\n",
    "        \n",
    "        schema = tiledb.ArraySchema(domain=dom, attrs=attr, sparse=False)\n",
    "        tiledb.Array.create(uri, schema)\n",
    "        tdb_array = tiledb.open(uri, \"w\")\n",
    "############################################################################################################################\n",
    "        \n",
    "        da.to_tiledb(tdb_array, storage_options={\"sm.compute_concurrency_level\": 2, \"sm.io_concurrency_level \": 2})\n",
    "    \n",
    "        # Consolidation is perfomed on the array for increased read speed from cloud object storage.\n",
    "        config['sm.consolidation.mode'] = 'fragment_meta'\n",
    "        ctx = tiledb.Ctx(config)\n",
    "        tiledb.consolidate(uri, ctx=ctx)\n",
    "        config['sm.consolidation.mode'] = 'fragments'\n",
    "        ctx = tiledb.Ctx(config)\n",
    "        tiledb.consolidate(uri, ctx=ctx)\n",
    "        config['sm.consolidation.mode'] = 'array_meta'\n",
    "        ctx = tiledb.Ctx(config)\n",
    "        tiledb.consolidate(uri, ctx=ctx)\n",
    "    \n",
    "    del ds, da, uri, dims, attr, dom, schema, tdb_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5763f",
   "metadata": {},
   "source": [
    "## Present Timing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd60cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diag_timer.dataframe()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH gcpslurmv2basic.clusters.pw gcpslurmv2basic-cloud-data",
   "language": "",
   "name": "rik_ssh_gcpslurmv2basic_clusters_pw_gcpslurmv2basicclouddata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
