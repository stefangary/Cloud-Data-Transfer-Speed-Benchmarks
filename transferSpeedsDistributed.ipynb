{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Parallel Data Read Speeds with Dask - SLURM Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dask.array as dsa\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from contextlib import contextmanager\n",
    "import xarray as xr\n",
    "import intake\n",
    "import time\n",
    "import dask\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.colors\n",
    "import pandas as pd\n",
    "from scipy.stats import sem\n",
    "import tiledb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slurm Job Script Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda Directory: /contrib/Jacob.Green/miniconda3 \n",
      "Conda Environment: cloud-data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "dask_dir = '/mnt/shared/dask'\n",
    "conda_dir = os.path.join('/contrib', os.environ['USER'],'miniconda3')\n",
    "conda_env = 'cloud-data'\n",
    "dask_port = 8786\n",
    "dashboard_port = 8787\n",
    "print('Conda Directory:', conda_dir, '\\nConda Environment:', conda_env)\n",
    "\n",
    "cluster = SLURMCluster(project='cg-cloudmgmt',\n",
    "                       cores=4, # Number of cores in the job\n",
    "                       memory='32GB', # Worker memory limit will be memory/processes\n",
    "                       processes=4, # Sets number of Dask workers. Threads per dask worker will be cores/processes\n",
    "                       name='gcpslurmv2basic', # Name of cluster\n",
    "                       queue='compute', # Partition name\n",
    "                       job_cpu=4, # Set this to the number of cpus per job\n",
    "                       job_mem='32GB', # Amount of memory per job\n",
    "                       walltime='01:00:00',\n",
    "                       log_directory=os.path.join(dask_dir, 'logs'),\n",
    "                       env_extra=[\n",
    "                           'source {conda_sh}; conda activate {conda_env}'.format(\n",
    "                           conda_sh = os.path.join(conda_dir, 'etc/profile.d/conda.sh'),\n",
    "                           conda_env= conda_env\n",
    "                           )\n",
    "                       ],\n",
    "                       header_skip=['--mem'],\n",
    "                      )\n",
    "\n",
    "client = Client(cluster)\n",
    "print('Job Script:\\n',cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevNullStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __setitem__(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "null_store = DevNullStore()\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "class DiagnosticTimer:\n",
    "    def __init__(self):\n",
    "        self.diagnostics = []\n",
    "        self.names = []\n",
    "        \n",
    "    @contextmanager\n",
    "    def time(self, **kwargs):\n",
    "        tic = time.time()\n",
    "        yield\n",
    "        toc = time.time()\n",
    "        kwargs[\"runtime\"] = toc - tic\n",
    "        self.diagnostics.append(kwargs)\n",
    "        \n",
    "    def dataframe(self):\n",
    "        return pd.DataFrame(self.diagnostics)\n",
    "    \n",
    "diag_timer = DiagnosticTimer()\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "def name(fileType, daf): \n",
    "    globals()[f\"df_{fileType}\"] = daf\n",
    "    diag_timer.names.append(globals()[f\"df_{fileType}\"])\n",
    "    \n",
    "    global df, da\n",
    "    del df, da\n",
    "    diag_timer.diagnostics = []\n",
    "    \n",
    "############################################################################################################################     \n",
    "\n",
    "def total_nthreads():\n",
    "    return sum([v for v in client.nthreads().values()])\n",
    "\n",
    "def total_ncores():\n",
    "    return sum([v for v in client.ncores().values()])\n",
    "\n",
    "def total_workers():\n",
    "    return len(client.ncores())\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "class mainLoop:\n",
    "    def errorCalc(self, df0):\n",
    "        global tests\n",
    "        newVals = []\n",
    "        info = []\n",
    "        thrPut = df0['throughput_Mbps']\n",
    "        rTime = df0['runtime']\n",
    "        for i in np.linspace(0, len(thrPut)-tests, int(len(thrPut)/tests), dtype='int'):\n",
    "            means = thrPut[slice(i,(i+tests))].mean()\n",
    "            runtime = rTime[slice(i,(i+tests))].mean()\n",
    "            errors = sem(thrPut[slice(i,(i+tests))])\n",
    "            error_kwargs = dict(runtime = runtime, throughput_Mbps = means, errors = errors)\n",
    "            info.append(df0.iloc[i, 0:7])\n",
    "            newVals.append(error_kwargs)\n",
    "        \n",
    "        df1 = pd.DataFrame(info, index=range(len(info)))\n",
    "        df2 = pd.DataFrame(newVals)\n",
    "        df = pd.concat([df1, df2], axis=1)\n",
    "        return df\n",
    "\n",
    "    def loop(self, da, diag_kwargs):\n",
    "        global tests, max_workers, worker_step\n",
    "        for nworkers in np.flip(np.arange(max_workers, 0, -worker_step)):\n",
    "            cluster.scale(nworkers)\n",
    "            time.sleep(10)\n",
    "            client.wait_for_workers(nworkers)\n",
    "            print('Number of Workers:', nworkers)\n",
    "            for i in range(tests):\n",
    "                with diag_timer.time(nworkers=total_workers(), nthreads=total_nthreads(), ncores=total_ncores(),\n",
    "                                     **diag_kwargs):\n",
    "                    future = dsa.store(da, null_store, lock=False, compute=False)\n",
    "                    dask.compute(future, retries=5)\n",
    "                del future\n",
    "        \n",
    "        df = diag_timer.dataframe()\n",
    "        df['throughput_Mbps'] = da.nbytes / 1e6 / df['runtime']\n",
    "        if i != 0:\n",
    "            df = self.errorCalc(df)\n",
    "        return df\n",
    "\n",
    "mainLoop = mainLoop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  User Input for Testing Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the user will define the testing benchmarking conditions:\n",
    "* **`tests = (int)`:** The number of times each individual file format will be read for measurement. Entering a number greater than 1 will take much longer to run, but results will include errors & throughput plot will have error bars.\n",
    "\n",
    "\n",
    "* **`max_workers = (int)`:** Maximum amount of parallel reads to be tested.\n",
    "\n",
    "\n",
    "* **`worker_step = (int)`:** Workers will be reduced by this number starting from the value of `max_workers` until the lowest possible value is reached. For instance, when `max_workers = 8` & `worker_step = 2`, the resulting worker scaling scheme will be `[2, 4, 6, 8]`.\n",
    "\n",
    "\n",
    "* **`root = (string)`:** Root uniform resource identifier (URI) of the object storage location. Can be changed to a public URL for public data.\n",
    "\n",
    "\n",
    "* **`data = (string)`:** The data set to test. Within the `gs://cloud-data-benchmarks` bucket, each file format begins with the same naming convention, with the only difference being the extension at the end of the file name -- e.g. `.nc`, `.zarr`, etc. If the user is providing their own data and bucket, ensure that the naming convention follows what was done for this notebook, or hardcode the object storage URIs in each applicable function call.\n",
    "\n",
    "Note: When using a data set that only has gridded formats available in cloud object storage, only run the **Gridded Data** section of the notebook. The **Tabluar Data** section will *not* work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop Parameters\n",
    "tests = 1\n",
    "max_workers = 8\n",
    "worker_step = 4 # Should be the same or a multiple of the number of processes you set in SLURMCluster(...)\n",
    "\n",
    "# Data Location\n",
    "root = 'gs://cloud-data-benchmarks/'\n",
    "data = 'slp.1948-2009.100MB'\n",
    "\n",
    "# Cloud Storage Access Token File\n",
    "token = '/contrib/Jacob.Green/cloud-data-benchmarks.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_csv(root + data + '.csv', assume_missing=True, names=['lon', 'lat', 'z'])\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='CSV', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('csv', df)\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_csv(root + data + '.50MB.partcsv/*', assume_missing=True, names=['lon', 'lat', 'z'])\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Partitioned CSV', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('partcsv', df)\n",
    "df_partcsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "df0 = dd.read_parquet(root + data + '.100MB.partparquet/*')\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "\n",
    "da = df0.to_dask_array(lengths=True)\n",
    "del df0\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Partitioned Parquet', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('partparquet', df)\n",
    "df_partparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data variables:\n",
       "    SLP      (TIME, LAT, LON) float32 ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intake.open_netcdf(root + data + '.nc',\n",
    "                   storage_options={'token':token}).to_dask().data_vars # Lists all data variables contained in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`variable = (string)` Choose a data variable from the list in the output above to use in read testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'SLP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "ds = intake.open_netcdf(root + data + '.nc', storage_options={'token':token}).to_dask()\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "\n",
    "# Set Dask chunks to match internal chunks\n",
    "internal_chunks = ds[variable].encoding['chunksizes']\n",
    "coords = ds[variable].dims\n",
    "da = ds[variable].chunk(chunks=dict(zip(coords, internal_chunks))).data\n",
    "\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "del ds\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='NetCDF', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('netcdf', df)\n",
    "df_netcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zarr Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "da = dsa.from_zarr(root + data + '.zarray', storage_options={'token':token})\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Zarr Array', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('zarray', df)\n",
    "df_zarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zarr Hierachical Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 6.09 GiB </td>\n",
       "                        <td> 99.45 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (90520, 94, 192) </td>\n",
       "                        <td> (22630, 24, 48) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 65 Tasks </td>\n",
       "                        <td> 64 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float32 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"156\" height=\"146\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"6\" x2=\"80\" y2=\"77\" />\n",
       "  <line x1=\"10\" y1=\"12\" x2=\"80\" y2=\"83\" />\n",
       "  <line x1=\"10\" y1=\"19\" x2=\"80\" y2=\"90\" />\n",
       "  <line x1=\"10\" y1=\"25\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"43\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"45\" y2=\"60\" />\n",
       "  <line x1=\"62\" y1=\"52\" x2=\"62\" y2=\"78\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 80.58823529411765,70.58823529411765 80.58823529411765,96.00085180870013 10.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"35\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"53\" y2=\"17\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"70\" y2=\"35\" />\n",
       "  <line x1=\"62\" y1=\"52\" x2=\"88\" y2=\"52\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"16\" y1=\"0\" x2=\"86\" y2=\"70\" />\n",
       "  <line x1=\"22\" y1=\"0\" x2=\"93\" y2=\"70\" />\n",
       "  <line x1=\"29\" y1=\"0\" x2=\"99\" y2=\"70\" />\n",
       "  <line x1=\"35\" y1=\"0\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 35.41261651458248,0.0 106.00085180870013,70.58823529411765 80.58823529411765,70.58823529411765\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"77\" x2=\"106\" y2=\"77\" />\n",
       "  <line x1=\"80\" y1=\"83\" x2=\"106\" y2=\"83\" />\n",
       "  <line x1=\"80\" y1=\"90\" x2=\"106\" y2=\"90\" />\n",
       "  <line x1=\"80\" y1=\"96\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"86\" y1=\"70\" x2=\"86\" y2=\"96\" />\n",
       "  <line x1=\"93\" y1=\"70\" x2=\"93\" y2=\"96\" />\n",
       "  <line x1=\"99\" y1=\"70\" x2=\"99\" y2=\"96\" />\n",
       "  <line x1=\"106\" y1=\"70\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.58823529411765,70.58823529411765 106.00085180870013,70.58823529411765 106.00085180870013,96.00085180870013 80.58823529411765,96.00085180870013\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"93.294544\" y=\"116.000852\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >192</text>\n",
       "  <text x=\"126.000852\" y=\"83.294544\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,126.000852,83.294544)\">94</text>\n",
       "  <text x=\"35.294118\" y=\"80.706734\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,80.706734)\">90520</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<open_dataset-a19d125c5c058d36c0d0cff4c47543f0SLP, shape=(90520, 94, 192), dtype=float32, chunksize=(22630, 24, 48), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic1 = time.time()\n",
    "ds = xr.open_zarr(store = root + data + '.zarr', consolidated=True, storage_options={'token':token})\n",
    "toc1 = time.time()\n",
    "connectTime = toc1-tic1\n",
    "da = ds[variable].data\n",
    "del ds\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Workers: 4\n",
      "Number of Workers: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nworkers</th>\n",
       "      <th>nthreads</th>\n",
       "      <th>ncores</th>\n",
       "      <th>nbytes</th>\n",
       "      <th>chunksize</th>\n",
       "      <th>format</th>\n",
       "      <th>connectTime</th>\n",
       "      <th>runtime</th>\n",
       "      <th>throughput_Mbps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>Zarr Group</td>\n",
       "      <td>0.539969</td>\n",
       "      <td>18.946113</td>\n",
       "      <td>344.916121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>Zarr Group</td>\n",
       "      <td>0.539969</td>\n",
       "      <td>8.561764</td>\n",
       "      <td>763.256287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nworkers  nthreads  ncores      nbytes  chunksize      format  connectTime  \\\n",
       "0         4         4       4  6534819840  104279040  Zarr Group     0.539969   \n",
       "1         8         8       8  6534819840  104279040  Zarr Group     0.539969   \n",
       "\n",
       "     runtime  throughput_Mbps  \n",
       "0  18.946113       344.916121  \n",
       "1   8.561764       763.256287  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='Zarr Group', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('zgroup', df)\n",
    "df_zgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TileDB Embedded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 6.09 GiB </td>\n",
       "                        <td> 99.45 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (90520, 94, 192) </td>\n",
       "                        <td> (22630, 24, 48) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 65 Tasks </td>\n",
       "                        <td> 64 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float32 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"156\" height=\"146\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"6\" x2=\"80\" y2=\"77\" />\n",
       "  <line x1=\"10\" y1=\"12\" x2=\"80\" y2=\"83\" />\n",
       "  <line x1=\"10\" y1=\"19\" x2=\"80\" y2=\"90\" />\n",
       "  <line x1=\"10\" y1=\"25\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"43\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"45\" y2=\"60\" />\n",
       "  <line x1=\"62\" y1=\"52\" x2=\"62\" y2=\"78\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 80.58823529411765,70.58823529411765 80.58823529411765,96.00085180870013 10.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"35\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"53\" y2=\"17\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"70\" y2=\"35\" />\n",
       "  <line x1=\"62\" y1=\"52\" x2=\"88\" y2=\"52\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"16\" y1=\"0\" x2=\"86\" y2=\"70\" />\n",
       "  <line x1=\"22\" y1=\"0\" x2=\"93\" y2=\"70\" />\n",
       "  <line x1=\"29\" y1=\"0\" x2=\"99\" y2=\"70\" />\n",
       "  <line x1=\"35\" y1=\"0\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 35.41261651458248,0.0 106.00085180870013,70.58823529411765 80.58823529411765,70.58823529411765\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"77\" x2=\"106\" y2=\"77\" />\n",
       "  <line x1=\"80\" y1=\"83\" x2=\"106\" y2=\"83\" />\n",
       "  <line x1=\"80\" y1=\"90\" x2=\"106\" y2=\"90\" />\n",
       "  <line x1=\"80\" y1=\"96\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"86\" y1=\"70\" x2=\"86\" y2=\"96\" />\n",
       "  <line x1=\"93\" y1=\"70\" x2=\"93\" y2=\"96\" />\n",
       "  <line x1=\"99\" y1=\"70\" x2=\"99\" y2=\"96\" />\n",
       "  <line x1=\"106\" y1=\"70\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.58823529411765,70.58823529411765 106.00085180870013,70.58823529411765 106.00085180870013,96.00085180870013 80.58823529411765,96.00085180870013\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"93.294544\" y=\"116.000852\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >192</text>\n",
       "  <text x=\"126.000852\" y=\"83.294544\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,126.000852,83.294544)\">94</text>\n",
       "  <text x=\"35.294118\" y=\"80.706734\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,80.706734)\">90520</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<tiledb-gs://cloud-data-benchmarks/slp.1948, shape=(90520, 94, 192), dtype=float32, chunksize=(22630, 24, 48), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = token\n",
    "tic1 = time.time()\n",
    "da = dsa.from_tiledb(root + data + '.tldb',\n",
    "                     storage_options={'sm.compute_concurrency_level': 4, 'sm.io_concurrency_level': 4})\n",
    "toc1 = time.time()\n",
    "connectTime = toc1 - tic1\n",
    "chunksize = np.prod(da.chunksize) * da.dtype.itemsize\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Workers: 4\n",
      "Number of Workers: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nworkers</th>\n",
       "      <th>nthreads</th>\n",
       "      <th>ncores</th>\n",
       "      <th>nbytes</th>\n",
       "      <th>chunksize</th>\n",
       "      <th>format</th>\n",
       "      <th>connectTime</th>\n",
       "      <th>runtime</th>\n",
       "      <th>throughput_Mbps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>TileDB Embedded</td>\n",
       "      <td>0.298915</td>\n",
       "      <td>17.318837</td>\n",
       "      <td>377.324400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6534819840</td>\n",
       "      <td>104279040</td>\n",
       "      <td>TileDB Embedded</td>\n",
       "      <td>0.298915</td>\n",
       "      <td>9.719304</td>\n",
       "      <td>672.354722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nworkers  nthreads  ncores      nbytes  chunksize           format  \\\n",
       "0         4         4       4  6534819840  104279040  TileDB Embedded   \n",
       "1         8         8       8  6534819840  104279040  TileDB Embedded   \n",
       "\n",
       "   connectTime    runtime  throughput_Mbps  \n",
       "0     0.298915  17.318837       377.324400  \n",
       "1     0.298915   9.719304       672.354722  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_kwargs = dict(nbytes=da.nbytes, chunksize=chunksize, format='TileDB Embedded', connectTime=connectTime)\n",
    "\n",
    "df = mainLoop.loop(da, diag_kwargs)\n",
    "name('tldb', df)\n",
    "df_tldb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class errorPlot:\n",
    "    def plot(self):\n",
    "        x = self.df['nworkers']\n",
    "        y = self.df['throughput_Mbps']\n",
    "        error = self.df['errors']\n",
    "        plt.errorbar(x, y, yerr=error, color=self.c, fmt='o', capsize=5, capthick=2)\n",
    "        \n",
    "    def errorCheck(self, daf, color):\n",
    "        self.c = color\n",
    "        self.df = daf\n",
    "        try:\n",
    "            self.plot()\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            self.plot()\n",
    "            \n",
    "errorPlot = errorPlot()\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "color = cm.rainbow(np.linspace(0,1,len(diag_timer.names)))\n",
    "legend = []\n",
    "df_results = pd.concat(diag_timer.names, ignore_index=True)\n",
    "\n",
    "for i in range(len(diag_timer.names)):\n",
    "    legend.append(diag_timer.names[i]['format'][1])\n",
    "    c = matplotlib.colors.to_hex(color[i,:], keep_alpha=True)\n",
    "    \n",
    "    if i == 0:\n",
    "        ax = diag_timer.names[i].plot(x='nworkers', y='throughput_Mbps', kind='line', color=c, marker='o')\n",
    "    else:\n",
    "        diag_timer.names[i].plot(x='nworkers', y='throughput_Mbps', kind='line', color=c, ax=ax, marker='o')\n",
    "        \n",
    "    errorPlot.errorCheck(diag_timer.names[i], c) \n",
    "    plt.grid(True)\n",
    "    plt.title('Cloud Data Read Speeds with Dask')\n",
    "    plt.xlabel('Number of Parallel Reads')\n",
    "    plt.ylabel('Throughput (Mbps)')\n",
    "    plt.legend(legend, bbox_to_anchor=[1.25, 0.5], loc='center', title='Store Formats')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    #plt.yscale('symlog') ACTIVATE THIS LINE IF YOU ARE USING A LARGE AMOUNT OF WORKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH gcpslurmv2basic.clusters.pw gcpslurmv2basic-dask-slurm-hello-world",
   "language": "",
   "name": "rik_ssh_gcpslurmv2basic_clusters_pw_gcpslurmv2basicdaskslurmhelloworld"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
